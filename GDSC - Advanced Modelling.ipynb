{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDSC Training Session - Advanced Modelling\n",
    "\n",
    "***\n",
    "<a id='Introduction'></a>\n",
    "# Introduction\n",
    "\n",
    "After creating our first simple model in the last training session we this time want to get into a more sophisticated technique to approach our task. This includes not only a more complex kind of model but also the steps to include bigger parts of our data into the training. This scale up will come in two ways:\n",
    "* include data from a longer time frame into the training\n",
    "* prepare data from different sensors as input for the model\n",
    "\n",
    "So, considering the general framework [CRISP-DM](https://www.sv-europe.com/crisp-dm-methodology/) we will this time focus on the Data Preparation and Modelling steps. Once again, we will not end up with the perfect model but provide further ideas how to improve on these aspects.\n",
    "\n",
    "Now, lets get started preparing the data and building our next model!!\n",
    "\n",
    "*Notes:*\n",
    "- *If this is your first Jupyter Notebook you might want to read [a general introduction](https://realpython.com/jupyter-notebook-introduction/) first.*\n",
    "- *Do not be afraid to ask (and answer) questions! Check out the [teams channel](https://teams.microsoft.com/l/team/19%3a4017a2e9af4942e7aa157d6ec9d751b4%40thread.skype/conversations?groupId=7d77d672-dff1-4c9f-ac55-3c837c1bebf9&tenantId=76a2ae5a-9f00-4f6b-95ed-5d33d77c4d61).*\n",
    "- *Material and recordings from last weeks session in this [teams post](https://teams.microsoft.com/l/message/19:4017a2e9af4942e7aa157d6ec9d751b4@thread.skype/1614516939664?tenantId=76a2ae5a-9f00-4f6b-95ed-5d33d77c4d61&groupId=7d77d672-dff1-4c9f-ac55-3c837c1bebf9&parentMessageId=1614516939664&teamName=Global%20Data%20Science%20Challenge%20-%20Public&channelName=General&createdTime=1614516939664).*\n",
    "\n",
    "## Table of contents:\n",
    "[1. Introduction](#Introduction) <br>\n",
    "[2. Recap - Task of the Challenge](#Recap) <br>\n",
    "[3. Data Preparation](#DataPrep) <br>\n",
    "&emsp; [3.1. Harmonizing the Sample Frequency](#Harmonize) <br>\n",
    "&emsp; [3.2. Data Stacking](#Stacking) <br>\n",
    "[4. Advanced Modelling](#Modelling) <br>\n",
    "&emsp; [4.1. Introduction](#ModelIntro) <br>\n",
    "&emsp; [4.2. Autoencoder Architecture](#Architecture) <br>\n",
    "&emsp; [4.3. Feeding big amounts of data to the model](#DataGenerator) <br>\n",
    "&emsp; [4.4. Leveraging Deep Leraning Frameworks on SageMaker](#Frameworks) <br>\n",
    "&emsp; [4.5. Creating a Training Job](#TrainingJob) <br>\n",
    "&emsp; [4.6. Inference](#Inference) <br>\n",
    "[5. Wrap Up](#Epilogue) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Recap'></a>\n",
    "# Recap - What we learned so far\n",
    "In last weeks session we performed exploratory data analysis (EDA) on the EK60 echosounder and point sensor data. We asked and answered questions about our data regarding the overall shape as well as created visualizations in order to manually gather insights from these two data sources.\n",
    "\n",
    "Also, we created a basic model based on a built-in AWS SageMaker algorithm - Random Cut Forrest (RCF) - to score our data points on their likelihood to be an anomaly. The decision if a scored datapoint is deemed an anomaly was based on a threshold we set after gathering some statistics on the distribution of the score values.\n",
    "\n",
    "Finally, we submitted the found anomalies as our first contribution to the Global Data Science Challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Recap'></a>\n",
    "# Data Preparation\n",
    "\n",
    "Since our task is to find anomalies across the various sensor readings we might not just look at the data of each sensor individually but find a way to combine it. This way we can provide it to a model which has the chance to pick up on correlations between the sensors. For example we would expect to see higher speed values near the ocean surface in the ADCP when there is more scatter on the ocean surface visible in the echogram due to increased wave activity. This could for example be due to the weather. This might also be picked up by the hydrophone. \n",
    "\n",
    "<a id='Harmonize'></a>\n",
    "## Harmonizing the Sample Frequency\n",
    "\n",
    "In order to combine the data one important step is to harmonize the sample frequency of the different sensors to a common sample rate. As you probably recall from our session last week and also validated in your own analysis this part has already been taken care of for the tabular data provided in S3. There we already have one data point per minute whereever data is available. Here again the overview of performed steps to get there:\n",
    "* ADCP\n",
    "    * Extraction of current speed and direction from the .nc files using [NetCDF4](https://pypi.org/project/netCDF4/) and [xarray](http://xarray.pydata.org/en/stable/)\n",
    "    * Linear interpolation of the data to provide one data point per minute sample rate - before roughly one every ten minutes\n",
    "    * Store data in CSV tables with one file per day of data and one data point per minute - speed and direction in one table, check the column names\n",
    "    \n",
    "* EK60 - Echosounder\n",
    "    * Excration of the volume backscatter strengh info from the .raw files using [PyEchoLab](https://github.com/CI-CMG/pyEcholab)\n",
    "    * Aggregation of the data using a mean to one data point per minute - before roughly one data point every 4 seconds\n",
    "    * Cutoff of values further away from the sensor than 300 meters, so data above the ocean surface\n",
    "    * Store data in CSV tables with one file per day of data and one data point per minute\n",
    "* Hydrophone\n",
    "    * Downsampling the .wav audio files to 12kHz maximum frequency using [librosa](https://librosa.org/doc/latest/index.html) - our research partners assured us that the information loss from this is negligable (Note: this is the state we provided to you in the *raw/* folder since the pure .wav files have an even bigger volume)\n",
    "    * Performing [Long Term Spectral Average](https://libraries.io/github/tryan/LTSA) on the data to have it available in the frequeny vs. time domain\n",
    "    * Store data in CSV tables with one file per day of data and one data point per minute\n",
    "* Point Sensors - Temperature, Salinity, Depth, ....\n",
    "    * Raw data already provided in CSV format with roughly hourly measurements\n",
    "    * Linear interpolation of the data to provide one data point per minute sample rate - before roughly one every hour\n",
    "    * Store data in CSV tables with one file per day of data and one data point per minute\n",
    "\n",
    "Since this first part of the data preparation was already performed within this tutorial we can come right away the the stacking of the data. This we will do in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stacking'></a>\n",
    "## Data Stacking\n",
    "We will now have a look at how to stack our data from the different sensors together to get one combined measurement point that contains the information of all our sensors. We will only show how it works for data of one day. Scaling this out for every day and possibly doing additional preprocessing on the data will be up to you.\n",
    "\n",
    "We start by downloading the CSV files from each sensor for a particular day. We can check the data availability for the day quickly via the AWS CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://gdsc4-eu/data/adcp/tabular/2015-05-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "\n",
    "def get_file_list_from_s3(bucket_name, prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix))\n",
    "    return [element.key for element in file_list]\n",
    "\n",
    "def download_file(bucket_name, file_path, local_output_path):\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    try:\n",
    "        s3.Bucket(bucket_name).download_file(\n",
    "            str(file_path), str(Path(local_output_path, file_name))\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"The object does not exist.\")\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just download the data we want to use for the stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"gdsc4-eu\"\n",
    "file_date = \"2015-05-25\"\n",
    "\n",
    "download_file(bucket, f\"data/adcp/tabular/{file_date}.csv\", Path(\"data/adcp\"))\n",
    "download_file(bucket, f\"data/ek60/tabular/{file_date}.csv\", Path(\"data/ek60\"))\n",
    "download_file(bucket, f\"data/hydrophone/tabular/{file_date}.csv\", Path(\"data/hydrophone\"))\n",
    "download_file(bucket, f\"data/point_sensors/tabular/{file_date}.csv\", Path(\"data/point_sensors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load all the CSV files into dataframe making sure we have a DatetimeIndex in place for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_date = \"2015-05-25\"\n",
    "adcp_data = pd.read_csv(Path(f\"data/adcp/{file_date}.csv\"), index_col=0, sep=',', parse_dates=True, header=0)\n",
    "ek60_data = pd.read_csv(Path(f\"data/ek60/{file_date}.csv\"), index_col=0, sep=',', parse_dates=True, header=0)\n",
    "hydrophone_data = pd.read_csv(Path(f\"data/hydrophone/{file_date}.csv\"), index_col=0, sep=',', parse_dates=True, header=0)\n",
    "point_sensors_data = pd.read_csv(Path(f\"data/point_sensors/{file_date}.csv\"), index_col=0, sep=',', parse_dates=True, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly check the format of each of the data sources again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adcp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ek60_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrophone_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_sensors_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a first glance most of the data looks good. The point sensors however seem to only contain info for turbidity. For now we will continue down our path to combine the data but this is an issue that needs to be adressed before the data is fed to a model for training, because most algorithms cannot handle this issue on their own. \n",
    "\n",
    "So let's join our data, and [join](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html#pandas.DataFrame.join) is the keyword here. Since pandas dataframes are basically tables very much like tables in databases we can make use of the methods that are very common in relational database interactions. All our tables have a common index, the timestamps. We can use this index to perform the join operation on. Since we have data available for each minute of the day in each table the kind of join does not matter in our particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sensor_data = adcp_data.join(ek60_data, how=\"outer\").join(hydrophone_data, how=\"outer\").join(point_sensors_data, how=\"outer\")\n",
    "all_sensor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exercise: What can you do to validate that the resulting data frame contains all the information (and also not more as) you want it to have? \n",
    "\n",
    "> Exercise: The approach described above assumes that the data from each sensor is available for a given day. As we have seen in our EDA, this is not the case for several periods within our time frame of interest. How can this be mitigated?\n",
    "\n",
    "> Exercise: try out different join methods (inner, outer, left, right, ...) when you have tables that do not contain 1440 data points per day and you therefore need to make up for the missing data in one of the sensor tables. What are the implications of the join method?\n",
    "\n",
    "This is already the end of our data preparation steps. We did not mitigate the obvious issues that arise from our data gaps or incomplete data on a particular day. Nonetheless this provide you some idea on how to tackle these problems. And do not forget, share your ideas and discuss with the community how to best approach this further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Modelling'></a>\n",
    "# Advanced Modelling\n",
    "\n",
    "<a id='ModelIntro'></a>\n",
    "## Introduction\n",
    "\n",
    "An [autocoder](https://en.wikipedia.org/wiki/Autoencoder) is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, so in a compressed form, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learned, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name.\n",
    "\n",
    "<img src=\"./notebook_images/Autoencoder_schema.png\" alt=\"autoencoder\" width=\"400\"/>\n",
    "\n",
    "The idea of autoencoders has been popular in the field of neural networks for decades. The first applications date to the 1980s. Their most traditional application was dimensionality reduction or feature learning, but the autoencoder concept became more widely used for learning generative models of data. Some of the most powerful AIs in the 2010s involved sparse autoencoders stacked inside deep neural networks.\n",
    "\n",
    "**So how does this relate to anomaly detection like we are aiming to do?**\n",
    "\n",
    "The mentioned application - dimensionality reduction - holds the key to this question. Once we are able to identify the main patterns the outliers can be revealed. For dimensionality reduction we could of course use tools like [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) but many traditional techniques like PCA suffer from the fact that they rely on linear transformations. In contrast, the autoencoder techniques can perform non-linear transformations and thus show their merits when the data problems are complex and non-linear.\n",
    "\n",
    "The autoencoder approach for our problem follows similar lines as we used for the RCF, just the way the anomaly score is determined changes. So what we want to do is the following:\n",
    "1. Train the autoencoder model to be able to reconstruct our data\n",
    "2. Provide our data to the model during inference and receive the reconstructed version\n",
    "3. Calculate the reconstruction error, which will serve as our \"anomaly score\"\n",
    "4. Analyze the reconstruction errors to define a threshold value\n",
    "5. Identify all data points that have a reconstruction error above the set threshold value\n",
    "\n",
    "This process hinges on the assumption that the trained autoencoder model will be able to very well reconstruct the data for normal situations, because it has seen lots of examples of those. The reconstructions of anomalies on the other hand will be worse and hence result in a higher reconstruction error.\n",
    "\n",
    "So let's start by looking at the overall network architecture before diving into some details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Architecture'></a>\n",
    "## Model Architecture\n",
    "The model architecture introduced here is quite simple. It can of course be further enhanced and tuned but for the tutorial here we want to keep it simple and only highlight the essentials for this approach. So here the implementation of the model architecture:\n",
    "\n",
    "```python\n",
    "def model(x_train, x_test, epochs, window_size, feature_dim, batch_size=1):\n",
    "    \"\"\"Generate a simple model\"\"\"\n",
    "    \n",
    "    input_data = keras.Input(shape=(window_size, feature_dim, 1), batch_size=batch_size)\n",
    "\n",
    "    x = layers.Conv2D(16, (4, 4), activation='relu', padding='same')(input_data)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "    x = layers.Conv2D(8, (4, 4), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(16, (4, 4), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoded = layers.Conv2D(1, (4, 4), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = keras.Model(input_data, decoded)\n",
    "    autoencoder.summary()\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    autoencoder.fit(x_train, epochs=epochs, validation_data=x_test)\n",
    "\n",
    "    return autoencoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation not only contains the definition of the model architecture but also sets the parameters necessary to compile the model and start the fitting of the model. For example we set our optimizer (Adam), the loss function (mean squarred error (mse)) to optimize and the for deep learning crucial hyperparameter for the learning rate.\n",
    "\n",
    "The input for the model will be our data points, hence the *feature_dim* parameter. Also, we want provide some context for a particular data point to the model. So we introduce a *window_size* which combines multiple data points that follow each other on the time dimension to one data point. This way we have a start to introduce the time dimension of our problem to the model. Finally, in deep learning you want to provide your model not with all the data at once and also not one data point at a time. Hence, the data is fed to the model in batches, smaller sample collections of your data (see this [blog](https://medium.com/analytics-vidhya/when-and-why-are-batches-used-in-machine-learning-acda4eb00763) for a few more details on the whens and whys of using batches). \n",
    "\n",
    "With a *window_size* of 64, a *batch_size* of 128 and *feature_dim* of 1600 this will yield the following structure:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param  \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(128, 64, 1600, 1)]      0         \n",
    "_________________________________________________________________\n",
    "conv2d (Conv2D)              (128, 64, 1600, 16)       272       \n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (128, 32, 800, 16)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (128, 32, 800, 8)         2056      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (128, 16, 400, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (128, 16, 400, 8)         1032      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (128, 8, 200, 8)          0         \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (128, 8, 200, 8)          1032      \n",
    "_________________________________________________________________\n",
    "up_sampling2d (UpSampling2D) (128, 16, 400, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (128, 16, 400, 8)         1032      \n",
    "_________________________________________________________________\n",
    "up_sampling2d_1 (UpSampling2 (128, 32, 800, 8)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (128, 32, 800, 16)        2064      \n",
    "_________________________________________________________________\n",
    "up_sampling2d_2 (UpSampling2 (128, 64, 1600, 16)       0         \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)            (128, 64, 1600, 1)        257       \n",
    "=================================================================\n",
    "Total params: 7,745\n",
    "Trainable params: 7,745\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a very small model but it highlights the basic principles of the autoencoder. Three layers of [convolutions](https://keras.io/api/layers/convolution_layers/convolution2d/) and [max-pooling](https://keras.io/api/layers/pooling_layers/max_pooling2d/) each for the encoder to get to the compressed data representation (i.e. with reduced dimensionality) followed by three layers of again convolution and [up-sampling](https://keras.io/api/layers/reshaping_layers/up_sampling2d/) each for the decoder (i.e. to reconstruct the data from the compressed representation) that provides us the same output dimensions as we provided as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not cover all the details of the model architecture as part of this tutorial. For this we can recommend the very good blog article on [Convolutional Autoencoders for Image Noise Reduction](https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763). In particular the section on \"How Does the Convolutional Autoencoders Work?\". For the technical aspects and implementation details we recommend to check the [Keras API reference](https://keras.io/api/) and also the [Tensorflow tutorial on autoencoders](https://www.tensorflow.org/tutorials/generative/autoencoder).\n",
    "  \n",
    "The model architecture however is only part of the picture. We also need to make sure our model is provided with the necessary input data and for a neural network we love to have big amounts of data to train on. On the next section we will see how this can be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DataGenerator'></a>\n",
    "## Feeding big amounts of data to the model\n",
    "In many tutorials and example implementations introducing you to machine learning and data science (like this generally very nice one on [timeseries anomaly detection using an Autoencoder](https://keras.io/examples/timeseries/timeseries_anomaly_detection/) or ) you will see that the data used to train the model is simply loaded **as a whole** into a Pandas dataframe or Numpy array and then provided to the model for training. This however assumes that the entire amount of data that is used can fit into memory on the machine that is used for training. Since the amount of data we are using is way too big to do so we will have to use a much smarter way to provide the training data to our model. The important point to realize is that the training process is never using all the data at once. It only needs parts of it at a time. Due to this fact TensorFlow/Keras provides the option of implementing a so called DataGenerator (see this nice [blog article](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) as an introduction) based on the [Keras Sequence](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) implementation. The DataGenerator can be passed to the fit method of the estimator and will squentially load only the part of the data that is currently needed for training and provide it during training.\n",
    "\n",
    "For our problem this DataGenerator will therefore load a certain number of rows of and across the daily CSV files. Here is the implementation to do so:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import boto3\n",
    "\n",
    "class DataGeneratorFromCSV(Sequence):\n",
    "    def __init__(self, file_names, window_size, feature_dim, batch_size, default_s3_location=\"data/ek60/tabular\"):\n",
    "        self.file_names = file_names\n",
    "        self.s3_location = default_s3_location\n",
    "        self.window_size = window_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.local_meta_info_path = Path(\"meta_info\")\n",
    "        self.local_meta_info_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.record_book = self.setup_record_book(file_names, default_s3_location)\n",
    "        \n",
    "        self.total_number_of_records = len(self.record_book)\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_record_book(file_names, s3_location):\n",
    "        \n",
    "        for file_name in file_names:\n",
    "            DataGeneratorFromCSV.download_file(\n",
    "                \"gdsc4-eu\", \n",
    "                f\"{s3_location}/meta_info_{file_name.stem}.txt\", \n",
    "                Path.cwd()\n",
    "            )\n",
    "        file_infos = {\n",
    "            file_name: DataGeneratorFromCSV.extract_number_of_records(\n",
    "                Path(Path.cwd(), f\"meta_info_{file_name.stem}.txt\")\n",
    "            ) for file_name in file_names\n",
    "        }\n",
    "        total_number_of_records = sum(file_infos.values())\n",
    "\n",
    "        record_table = {\n",
    "            'overall_record_id': [entry for entry in range(0, total_number_of_records)],\n",
    "            'file_record_id': list(itertools.chain.from_iterable(\n",
    "                [range(0, file_infos[file_name]) for file_name in file_infos.keys()]\n",
    "            )),\n",
    "            'file_name': list(itertools.chain.from_iterable(\n",
    "                [itertools.repeat(file_name, file_infos[file_name]) for file_name in file_infos.keys()]\n",
    "            )),\n",
    "        }\n",
    "        record_book = pd.DataFrame.from_dict(record_table)         \n",
    "        return record_book.set_index('overall_record_id')\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.floor(\n",
    "            self.total_number_of_records / (self.batch_size * self.window_size)\n",
    "        ) - 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        from_record = index * self.batch_size * self.window_size\n",
    "        to_record = (index + 1) * self.batch_size * self.window_size\n",
    "        csv_files = self.record_book[from_record:to_record][\"file_name\"].unique()\n",
    "        from_record_info = self.record_book.iloc[from_record]\n",
    "        to_record_info = self.record_book.iloc[to_record]\n",
    "        \n",
    "        if len(csv_files) == 1:\n",
    "            df = pd.read_csv(from_record_info.file_name)\n",
    "            df = df[from_record_info.file_record_id:to_record_info.file_record_id]\n",
    "            batch = self.perform_preprocessing(df)\n",
    "            batch = batch.reshape([self.batch_size, self.window_size, self.feature_dim, 1])\n",
    "        else:\n",
    "            data_collection = []\n",
    "            for csv_file in csv_files:\n",
    "                data = pd.read_csv(csv_file)\n",
    "                data.columns = [f\"{el}\" for el in range(0, data.shape[1])] # enforce exactly matching column names\n",
    "                if csv_file == csv_files[0]:\n",
    "                    data = data[from_record_info.file_record_id:]\n",
    "                elif csv_file == csv_files[-1]:\n",
    "                    data = data[:to_record_info.file_record_id]\n",
    "                else:\n",
    "                    data = data[:]\n",
    "                data_collection.append(data)\n",
    "            \n",
    "            df = pd.concat(data_collection)\n",
    "            batch = self.perform_preprocessing(df)\n",
    "            batch = batch.reshape([self.batch_size, self.window_size, self.feature_dim, 1])\n",
    "\n",
    "        batch = np.asarray(batch).astype('float32')\n",
    "        return batch, batch\n",
    "    \n",
    "    def get_file_path(self, index):\n",
    "        return self.file_names[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_preprocessing(df):\n",
    "        df = df.fillna(0).to_numpy()[:, 1:] \n",
    "        df /= np.max(np.abs(df), axis=0)\n",
    "        return df\n",
    "              \n",
    "    @staticmethod\n",
    "    def extract_number_of_records(meta_info_file_path):\n",
    "        with open(Path(meta_info_file_path), \"r\") as f:\n",
    "            text = f.read()\n",
    "        return int(text.split('\\n')[1].split('=')[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_file(bucket_name, file_path, local_output_path):\n",
    "        s3 = boto3.resource('s3')\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        try:\n",
    "            s3.Bucket(bucket_name).download_file(\n",
    "                str(file_path), str(Path(local_output_path, file_name))\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(\"The object does not exist.\")\n",
    "            else:\n",
    "                raise\n",
    "```\n",
    "\n",
    "There are several things to note here:\n",
    "\n",
    "Based on the Keras Sequence class we have two methods that are required to be implemented: \n",
    "* \\_\\_getitem\\_\\_ method - this method is responsible for providing a batch of data points when called. Hence it loads the chunk of the data from the different CSV files and brings them together in a batch. As a return value it provides (batch, batch), so the same data for input and target, which is due to the nature of the auto encoder model we want to train based on the data provided by the DataGenerator\n",
    "* \\_\\_len\\_\\_ method - this method simply calculates the number of batches that can be created from the overall data that is at the DataGenerators disposal. Here our meta_info files come in handy once more.\n",
    "\n",
    "Let's briefly see these two in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(bucket, f\"data/ek60/tabular/2015-03-01.csv\", Path(\"data/ek60\"))\n",
    "download_file(bucket, f\"data/ek60/tabular/2015-03-02.csv\", Path(\"data/ek60\"))\n",
    "download_file(bucket, f\"data/ek60/tabular/2015-03-03.csv\", Path(\"data/ek60\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import DataGeneratorFromCSV\n",
    "file_names = [Path(\"data/ek60\", el) for el in ['2015-03-01.csv', '2015-03-02.csv', '2015-03-03.csv']]\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGeneratorFromCSV(file_names, window_size=8, feature_dim=1600, batch_size=32)\n",
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator.__getitem__(1)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the required methods we also added other methods for our convenience and for keeping track of which data points from the files to use for a batch:\n",
    "* setup_record_book method - the output provided by this method is a table that contains then necessary information to keep track of the data points in play and ensure the right ones are selected from the different files for each batch. It contains a gloabl index for the data points, the index of the data point within each file as well as the file name in which the data point is stored.\n",
    "* perform_preprocessing method - as the name says this method performs preprocessing operations on our data. It currently simply fills NA values with 0 and create a numpy array from the dataframe but in general it can be extended to perform further cleanup and data selection to improve model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Frameworks'></a>\n",
    "## Leveraging SageMaker for Deep Learning Frameworks\n",
    "\n",
    "The parts described so far can be used on your local machine to train a model using TensorFlow. But we aim to train our model on AWS SageMaker to leverage the compute and storage capabilities that make this service so powerful. In order to do this we need to wrap this code into a script that fulfills some basic formalities. In this way you can make deep learning frameworks like TensorFlow, PyTorch or MXNet run on AWS SageMaker. This is done using the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html). The code snippets necessary to provide the connection between your \"local\" implementation and the format SageMaker needs can be found [here](https://sagemaker.readthedocs.io/en/stable/overview.html#prepare-a-training-script). For us it looks as follows:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "    parser.add_argument(\n",
    "        '--epochs',\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='The number of steps to use for training.')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help='Batch size for training.')\n",
    "    parser.add_argument(\n",
    "        '--window-size',\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help='Window size - number of records to process as one.')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args, unknown = _parse_args()\n",
    "    local_train_data_path = args.train\n",
    "    train_data_file_list = sorted(\n",
    "        list(Path(local_train_data_path).glob('**/*.csv')), \n",
    "        key=lambda file_path: file_path.stem\n",
    "    )\n",
    "    data_generator = DataGeneratorFromCSV(\n",
    "        train_data_file_list, \n",
    "        window_size=args.window_size, \n",
    "        feature_dim=1600, \n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    autoencoder = model(\n",
    "        data_generator, \n",
    "        data_generator, \n",
    "        args.epochs, \n",
    "        window_size=args.window_size, \n",
    "        feature_dim=1600, \n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    if args.current_host == args.hosts[0]:\n",
    "        # save model to an S3 directory with version number '00000001' in Tensorflow SavedModel Format\n",
    "        # To export the model as h5 format use model.save('my_model.h5')\n",
    "        autoencoder.save(os.path.join(args.sm_model_dir, '000000001'))\n",
    "\n",
    "```\n",
    "\n",
    "In the *\\_parse_args()* method we have the environment variables that are provided by SageMaker, including the hyperparameters we can provide to our script. Their individual meaning is explained in the [docs](https://sagemaker.readthedocs.io/en/stable/overview.html#prepare-a-training-script) in more detail. The parsing happens in its usual Python boilerplate fashion. \n",
    "\n",
    "In the *\\_\\_main\\_\\_* section we then only implement the few necessary steps to start the model training. SageMaker already downloaded the training data from S3 to the training instance's local hard drive and from there we get the file paths of the CSV files. We use them to instantiate our DataGenerator and then provide the generator as input as well as output to the model. Finally the trained autoencoder model is saved to disk from where SageMaker will copy it over to S3.\n",
    "\n",
    "> Exercise: \n",
    "\n",
    "> In this implementation some parameters are still hard coded: \n",
    "> * bucket name\n",
    "> * feature_dim of the data\n",
    "\n",
    "> Make them parameters that you can provide via the SageMaker Estimator call instead of within the *autoencoder.py* file.\n",
    "\n",
    "Now we have all the pieces in place and go on to the few lines of code still remaining to start the training job to actually create our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TrainingJob'></a>\n",
    "## Creating the model\n",
    "\n",
    "For model training we follow the same steps as with the Random Cut Forest model. The main difference comes from the fact that we are using not a built-in algorithm but instead rely on Tensorflow for creating the estimator object. When using Tensorflow on AWS SageMaker you need to provide an \"entry_point\" file that includes your code with the steps necessary to train the model. The nice part about this is, that the code you develop for this entry point file is very much the same as you would do when using Tensorflow on your local machine. The code snippets we discussed so far - model architecture, data generator and the necessary formalities in the \\_\\_main\\_\\_ scope - form the \"entry_point\" file we will provide to the AWS SageMaker Tensorflow estimator. So in the following section we will focus on the remaining surrounding parts to start a SageMaker training job using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Tensorflow Model on AWS SageMaker\n",
    "\n",
    "Similar to the RCF approach we can start the SageMaker machinery and the set the parameters regarding our training data and hyperparameters.\n",
    "\n",
    "To test and develop this we recommend using the so called **local mode**, which will start docker containers on your SageMaker Notebook/Studio instance to simulate the training instance without all the backend overhead and thus allows much faster development feedback cycles. How to use it is described in the [docs](https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode) and this [example](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/chainer_mnist/chainer_mnist_local_mode.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'gdsc4-eu'\n",
    "prefix = 'data/ek60/tabular/2015'\n",
    "training_data_uri = f's3://{bucket}/{prefix}'\n",
    "\n",
    "batch_size = 128\n",
    "window_size = 64\n",
    "epochs = 4\n",
    "\n",
    "hyperparameters = {'epochs': epochs, 'batch-size': batch_size, 'window-size': window_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the estimator object using the SageMaker Tensorflow implemenation. Notice again the need to set \"role\" but also the possibility to set \"instance_count\", \"instance_type\", \"hyperparameters\" and the essential \"entry_point\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_estimator = TensorFlow(\n",
    "    entry_point='autoencoder.py',\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    framework_version='2.3.1',\n",
    "    image_uri='763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">CAUTION: DO NOT EXECUTE THIS NEXT CELL UNLESS YOU HAVE PLENTY OF TIME (~1 HOUR) ON YOUR HANDS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model= autoencoder_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process should be monitored in order to see whether your model is actually training, so improving from a theoretical point of view, e.g. via the value of the loss function decreasing. This can be done via the logs, collected via [AWS CloudWatch](https://aws.amazon.com/cloudwatch/?nc1=h_ls). For the training jobs you ran the logs can be reached via the SageMaker Console.\n",
    "\n",
    "> Exercise: during training we can collect important information about the learning progress of our model. Tensorboard is a very good way to collect and visualize those metrics. Setup TensorBoard to have an easy way to monitor your training jobs progress\n",
    "\n",
    "> Hint: an example can be found in this [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorboard_keras/tensorboard_keras.ipynb) and this [blog article](https://aws.amazon.com/blogs/machine-learning/visualizing-tensorflow-training-jobs-with-tensorboard/)    \n",
    "\n",
    "To improve your training result you can experiment with techniques like [early stopping](https://keras.io/api/callbacks/early_stopping/). As the name suggests \"early stopping\" tells trainig job to stop once the model is not getting better with additional steps.\n",
    "\n",
    "> Exercise: implement a call back for early stopping to get the best out of your training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Inference'></a>\n",
    "## Inference\n",
    "\n",
    "With our model trained and stored in S3 we now want to deploy the model to create an endpoint for us to use for predictions. In the case of our autoencoder model this means we will use the endpoint to have it reconstruct our input data.\n",
    "\n",
    "To create the endpoint we will need the necessary execution role to grant the needed permissions for our operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you trained your model some time before and just want to use it now, e.g. to create and endpoint out of it, you need to add the following step into your workflow. You create the [TensorflowModel](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model) object from the information stored on S3 during training. The S3 path for the *model_data* parameter can be seen in at the bottom of the training job overview in the Sagemaker Console -> Training -> Trainig Jobs -> \\<Training Job Name\\>.\n",
    "> Exercise: find a way to get this information programmatically via the [AWS Python SDK (boto3)](https://aws.amazon.com/sdk-for-python/?nc1=h_ls)\n",
    "\n",
    "The *image_uri* depends on the used framework and version. An overview of the available deep learning containers can be found [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#general-framework-containers). Note that the region needs to be adjusted and also keep an eye out for the difference on training and inference containers.\n",
    "\n",
    "**CAUTION: ONLY EXECUTE THE FOLLOWING CELL IF YOU WANT TO USE A PREVIOUSLY TRAINED MODEL TO CREATE THE ENDPOIND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    model_data='s3://sagemaker-eu-west-1-435914187013/tensorflow-training-2021-03-01-01-55-35-932/output/model.tar.gz', \n",
    "    image_uri='763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:2.3.1-cpu-py37-ubuntu18.04',\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New we simply call the deploy method on our model to create the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.c5.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an already existing Endpoint\n",
    "In case you or your team mates have already deployed an endpoint before, you can reuse it in the following way by creating a [TensorFlowPredictor](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-predictor) object.\n",
    "\n",
    "We start by listing the existing endpoints using the [AWS Python SDK (boto3)](https://aws.amazon.com/sdk-for-python/?nc1=h_ls). You can also check the available endpoints using the browser by navigating to SageMaker Console -> Inference -> Endpoints. Once we got the name of the endpoint we can use it to create the predictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "\n",
    "client = boto3.client('sagemaker') \n",
    "\n",
    "endpoints = client.list_endpoints(\n",
    "    StatusEquals='InService'\n",
    ")\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = endpoints['Endpoints'][0]['EndpointName']\n",
    "print(f\"Name of running endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TensorFlowPredictor(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our predictor, let's get the data we want to use for the predictions, which in this case means we will let the predictor recreate the input, i.e. create a reconstruction. In order to do so we need to bring the data into the same shape as we fed it to the model during training. So we load the data, perform the preprocessing and bring it into the right shape from an input data perspective but also in terms of what the endpoint expects as a format.\n",
    "\n",
    "We will use again the convenience methods we already know to interact with the S3 bucket programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "\n",
    "def get_file_list_from_s3(bucket_name, prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    file_list = list(bucket.objects.filter(Prefix=prefix))\n",
    "    return [element.key for element in file_list]\n",
    "\n",
    "def download_file(bucket_name, file_path, local_output_path):\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    try:\n",
    "        s3.Bucket(bucket_name).download_file(\n",
    "            str(file_path), str(Path(local_output_path, file_name))\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"The object does not exist.\")\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just download the data we want to use for the prediction. This is only a small sample, for creating predictions on the full dataset we recommend to look into [batch transform jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-model-deployment.html#ex1-batch-transform). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"gdsc4-eu\"\n",
    "prefix_data_2015_02 = \"data/ek60/tabular/2015-02\"\n",
    "local_ek60_data_path = Path(\"data/ek60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2015_02 = get_file_list_from_s3(bucket, prefix_data_2015_02)\n",
    "for s3_file_path in files_2015_02:\n",
    "    download_file(bucket, s3_file_path, local_ek60_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load the data of a particular file (we will use the same one as in last weeks session to also compare the results between the two approaches) and then perform the identical preprocessing steps from the model training. For this we use a method that similar to the one used in the DataGenerator implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = Path(\"data/ek60/2015-02-22.csv\")\n",
    "window_size = 8\n",
    "feature_dim = 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file, index_col=0, sep=',', parse_dates=True, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are used for the data preparation within the prediction step and also to calculate the reconstruction error, the anologon to the anomaly score we saw last week for the random cut forest (RCF) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_datapoint(index, df, window_size, feature_dim=1600):\n",
    "    df = df[index : index + window_size]\n",
    "    data = perform_preprocessing(df)\n",
    "    return data.reshape([1, window_size, feature_dim, 1])\n",
    "\n",
    "def perform_preprocessing(df):\n",
    "    data = df.fillna(0).to_numpy() \n",
    "    data /= np.max(np.abs(data), axis=0)\n",
    "    return data\n",
    "\n",
    "def calculate_mean_squared_error(original, reconstruction):\n",
    "    return (np.square(original - reconstruction)).mean(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the reconstructions for each timestamp by iterating over the dataframe. Each timestamp is represented by its own data plus the timestamps lying ahead according to the used *window_size*. The input format for the endpoint follows the TensorFlow Serving REST API. How this looks can be checked [here](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#making-predictions-against-a-sagemaker-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"timestamps\": list([]),\n",
    "    \"reconstruction_errors\": list([]),\n",
    "}\n",
    "\n",
    "for i in range(0, df.shape[0] - window_size):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Reconstructing datapoint {i + 1} out of {df.shape[0] - window_size}\")\n",
    "    data_point = create_input_datapoint(i, df, window_size, feature_dim)\n",
    "    prediction = predictor.predict({'instances': [data_point.tolist()]})\n",
    "    reconstructed_data_point = np.array(prediction['predictions'])\n",
    "    mse = calculate_mean_squared_error(data_point, reconstructed_data_point)\n",
    "    results[\"reconstruction_errors\"].append(mse)\n",
    "    results[\"timestamps\"].append(df.index[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on the steps should be very familiar since they are the same ones we performed last week for the RCF model to post-process its results and manually validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "        {\"reconstruction_errors\": results[\"reconstruction_errors\"]}, \n",
    "        index=pd.DatetimeIndex(results[\"timestamps\"]),\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean = results_df.mean()\n",
    "score_std = results_df.std()\n",
    "anomaly_score_threshold = score_mean + 2 * score_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = results_df[results_df[\"reconstruction_errors\"].values > anomaly_score_threshold.values]\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ek60_file = Path(\"data/ek60/2015-02-22.csv\") \n",
    "ek60_inference_data = pd.read_csv(\n",
    "    ek60_file,\n",
    "    index_col=0,\n",
    "    sep=',',\n",
    "    parse_dates=True,\n",
    "    header=0,\n",
    ")\n",
    "ek60_inference_range = [float(column_name.split(\"_\")[2].split(\"m\")[0]) for column_name in ek60_inference_data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook kernel we are using matplotlib is not installed by default. So we need to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(25,10))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.pcolormesh(\n",
    "    pd.to_datetime(ek60_inference_data.index),\n",
    "    ek60_inference_range,\n",
    "    ek60_inference_data.to_numpy().T,\n",
    "    cmap=plt.get_cmap('plasma'),\n",
    "    vmin=-90,\n",
    "    vmax=-30\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"Time\")\n",
    "ax1.set_ylabel(\"Meters\")\n",
    "\n",
    "ax2.plot(\n",
    "    results_df[\"2015-02-22 00:00\": \"2015-02-22 23:59\"].index,\n",
    "    results_df[\"2015-02-22 00:00\": \"2015-02-22 23:59\"][\"reconstruction_errors\"],\n",
    "    color=\"whitesmoke\",\n",
    "    linewidth=0.4,\n",
    ")\n",
    "ax2.scatter(\n",
    "    anomalies[\"2015-02-22 00:00\": \"2015-02-22 23:59\"].index, \n",
    "    anomalies[\"2015-02-22 00:00\": \"2015-02-22 23:59\"][\"reconstruction_errors\"], \n",
    "    color=\"black\"\n",
    ")\n",
    "ax2.set_ylabel(\"Anomaly Score\")\n",
    "\n",
    "fig.suptitle(\"EK60 Echosounder - Anomaly Candidates in Echogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the session last week we added the anomaly scores (this time called reconstruction error) over time on top of the echogram. The thin grey time series indicates the anomaly score provided by the autoencoder model. Anomaly scores with black dots indicate an anomaly score above our threshold. Those are of particular interest and we want to check if at this time we also see an interesting structure in the echogram. Also we want to validate if the anomaly score values (their tendency to be lower or higher) match our human perception of \"things being out of the ordinary in the echogram\".\n",
    "\n",
    "This time it looks like bad news on a first glance for our first attempt to train an autoencoder model to detect the anomalies in our data. There is an increase in the reconstruction error/anomaly score for the time with increase activity at the ocean surface but this seems not to be enough to be seen as an outlier. The brighter sprincles for the most part also increase the reconstruction error, but also by far not enough to be classified as an outlier. However, the statistics for the threshold were only calculated on this tiny part of the data.\n",
    "\n",
    "Nonetheless, this means back to EDA and further analyzing the model results to gain better understanding on the model outputs!\n",
    "\n",
    "> Exercise: We only used one day of inference data to compute our statistics for setting the threshold. Scale this out and recheck the findings for this echogram - are more data points you would consider anomolous classified as such by the model now?\n",
    "\n",
    "> Exercise: Visualize the original input data vs. the reconstruction in order to gain insights whether our model has problems in general or with particular structures in the data. Hint: check the blog article on [Convolutional Autoencoders for Image Noise Reduction](https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763) for a way how to do this\n",
    "\n",
    "> Exercise: Try improving the model performance by e.g. \n",
    "> * using a pre-trained CNN for the encoder part\n",
    "> * introducing regularization in your model via [drop-out layers](https://keras.io/api/layers/regularization_layers/dropout/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">BEFORE WE FINISH UP, MAKE SURE TO STOP YOUR ENDPOINT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Epilogue'></a>\n",
    "# Wrap Up\n",
    "\n",
    "## Recap\n",
    "In today's session we looked into a possible way to combine the data sets from each sensor to produce a comprehensive data point containing all the information at hand for the given point in time.\n",
    "\n",
    "Our main focus however was to introduce the steps necessary to train a convolutional autoencoder model using TensorFlow and using it for inference by reconstructing our input data and computing the reconstruction error to detect anomalies in our data. This covered the steps: \n",
    "* defining the model architecture with its different layers\n",
    "* setting up the data generator to fed bigger amounts of data to the model\n",
    "* wrapping the TensorFlow code in a format necessary for AWS SageMaker\n",
    "* creating the SageMaker training job\n",
    "* creating the endpoint - right from the just trained model or by using a previously trained model\n",
    "* reconstructing our data using the inference endpoint\n",
    "* calculating the reconstruction error and setting the threshold value\n",
    "* identifying potential anomalous data points\n",
    "\n",
    "## Epilogue\n",
    "This is the end of the tutorial sessions for the Global Data Science Challenge. We hope you had fun following along with the tutorials, learned some new skills and are now eager to experiment with the data leveraging the introduced concepts. We wish you best of luck in the challenge! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
